{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nveldt/.julia/conda/3/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gensim\n",
    "from gensim.utils import tokenize\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, stem_text, strip_multiple_whitespaces, strip_tags, strip_short, strip_punctuation\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from string import ascii_lowercase\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../fauci-email-data.json\") as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_replacement_dict = {\n",
    "    \"fauc\": \"fauci\",\n",
    "    \"covi\": \"covid\",\n",
    "    \"coro\": \"corona\",\n",
    "    \"coron\": \"corona\",\n",
    "    \"corona\": \"corona\",\n",
    "    \"coronav\": \"coronavirus\",\n",
    "    \"coronavi\": \"coronavirus\",\n",
    "    \"coronavir\": \"coronavirus\",\n",
    "    \"coronavirn\": \"coronavirus\",\n",
    "    \"coronaviru\": \"coronavirus\",\n",
    "    \"coronavirus\": \"coronavirus\",\n",
    "    \"coronoviru\": \"coronavirus\",\n",
    "    \"covtd\": \"covid\",\n",
    "    \"econom\": \"economi\",\n",
    "    \"editori\": \"editoria\",\n",
    "    \"edito\": \"editor\",\n",
    "    \"forwa\": \"forward\",\n",
    "    \"globa\": \"global\",\n",
    "    \"healt\": \"health\",\n",
    "    \"instit\": \"institut\",\n",
    "    \"institu\": \"institut\",\n",
    "    \"internationa\": \"internation\",\n",
    "    \"interviewon\": \"interview\",\n",
    "    \"lnstagram\": \"instagram\",\n",
    "    \"orig\": \"origin\",\n",
    "    \"origi\": \"origin\",\n",
    "    \"quarant\": \"quarantin\",\n",
    "    \"reat\": \"treat\",\n",
    "    \"reatment\": \"treatment\",\n",
    "    \"strateg\": \"strategi\",\n",
    "    \"wro\": \"wrote\",\n",
    "    \"wrot\": \"wrote\",\n",
    "    \"iaid\": \"niaid\",\n",
    "    \"viru\": \"virus\",\n",
    "    \"foryour\": \"your\",\n",
    "    \"nlaid\": \"niaid\"\n",
    "}\n",
    "\n",
    "def dedup_replace(word): return stem_replacement_dict.get(word, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [strip_tags,\n",
    "           strip_punctuation,\n",
    "           strip_multiple_whitespaces, \n",
    "           remove_stopwords, \n",
    "           strip_short, \n",
    "           stem_text]\n",
    "\n",
    "def clean_text(input_text):\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in input_text.split(\"\\n\"):\n",
    "        tokenized = \" \".join(list(tokenize(sentence, lowercase=True)))\n",
    "        processed_str = preprocess_string(tokenized) #, filters)\n",
    "        if len(processed_str) > 0:\n",
    "            cleaned_sentences.append([dedup_replace(w) for w in processed_str])\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = data[\"emails\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = []\n",
    "for (i, chain) in enumerate(emails):\n",
    "    for email in chain:\n",
    "        clean_subj = clean_text(email[\"subject\"])\n",
    "        email[\"clean_subj\"] = clean_subj\n",
    "        if len(clean_subj) > 0:\n",
    "            for cs in clean_subj:\n",
    "                all_text.append(cs)\n",
    "\n",
    "        clean_body = clean_text(email[\"body\"])\n",
    "        email[\"clean_body\"] = clean_body\n",
    "        if len(clean_body) > 0:\n",
    "            for cb in clean_body:\n",
    "                all_text.append(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Word2Vec(all_text, vector_size=50, min_count=10) # pretty good!\n",
    "model = Word2Vec(all_text, vector_size=32, min_count=12,\n",
    "                 alpha=0.025, min_alpha=0.0001,\n",
    "                 epochs=8,\n",
    "                 compute_loss=True\n",
    "                 )\n",
    "#len(model.wv)\n",
    "#model.get_latest_training_loss() / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fauci-email-w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('toni', 0.878736674785614),\n",
       " ('franci', 0.8447346687316895),\n",
       " ('colleagu', 0.8063818216323853),\n",
       " ('collin', 0.7924655675888062),\n",
       " ('sir', 0.7867681980133057),\n",
       " ('prof', 0.7852154970169067),\n",
       " ('sun', 0.7787708044052124),\n",
       " ('david', 0.7730051875114441),\n",
       " ('avi', 0.7680595517158508),\n",
       " ('webbi', 0.7609789967536926)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"fauci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.9305621981620789),\n",
       " ('doctor', 0.8902467489242554),\n",
       " ('hall', 0.8899942636489868),\n",
       " ('virtual', 0.8759146332740784),\n",
       " ('head', 0.8754397034645081),\n",
       " ('member', 0.8742878437042236),\n",
       " ('told', 0.872067928314209),\n",
       " ('confer', 0.8711934685707092),\n",
       " ('town', 0.8627811074256897),\n",
       " ('council', 0.86131352186203)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vice', 0.9026037454605103),\n",
       " ('execut', 0.8942199349403381),\n",
       " ('director', 0.8809024691581726),\n",
       " ('senior', 0.8788819909095764),\n",
       " ('deputi', 0.8718844652175903),\n",
       " ('chief', 0.8537507653236389),\n",
       " ('harvard', 0.845360517501831),\n",
       " ('maryland', 0.8336153626441956),\n",
       " ('univers', 0.813807487487793),\n",
       " ('dean', 0.8108547329902649)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"presid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('phase', 0.8731111884117126),\n",
       " ('treatment', 0.8536596298217773),\n",
       " ('develop', 0.8478230834007263),\n",
       " ('influenza', 0.8452311158180237),\n",
       " ('antibodi', 0.842274010181427),\n",
       " ('strategi', 0.8387377262115479),\n",
       " ('vaccin', 0.8261803388595581),\n",
       " ('ncov', 0.8245812058448792),\n",
       " ('studi', 0.8193559646606445),\n",
       " ('wuhan', 0.8079530000686646)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('youtub', 0.9924458861351013),\n",
       " ('facebook', 0.9811912178993225),\n",
       " ('googl', 0.9796998500823975),\n",
       " ('nytim', 0.971889317035675),\n",
       " ('doi', 0.9712660908699036),\n",
       " ('nyt', 0.9672459363937378),\n",
       " ('podcast', 0.9646225571632385),\n",
       " ('bit', 0.9638143181800842),\n",
       " ('mike', 0.9636254906654358),\n",
       " ('url', 0.9635125398635864)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted([w for w in model.wv.index_to_key])\n",
    "with open(\"words.txt\", \"w\") as f:\n",
    "    for word in words:\n",
    "        f.write(word)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(model.wv[\"fauci\"])\n",
    "\n",
    "def mean_emb_vec(sentences):\n",
    "    all_words = [word for sentence in sentences for word in sentence]    \n",
    "    emb = np.zeros(dim)\n",
    "    num_words = 0\n",
    "    for word in all_words:\n",
    "        if word in model.wv:\n",
    "            emb += model.wv[word]        \n",
    "            num_words += 1\n",
    "            \n",
    "    if num_words == 0:\n",
    "        return emb\n",
    "    return emb / num_words\n",
    "\n",
    "\n",
    "for (i, chain) in enumerate(emails):\n",
    "    for email in chain:\n",
    "        email[\"subject_embedding\"] = list(mean_emb_vec(email[\"clean_subj\"]))\n",
    "        email[\"body_embedding\"] = list(mean_emb_vec(email[\"clean_body\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_emails = []\n",
    "for chain in emails:\n",
    "    embedded_chain = []\n",
    "    for email in chain:\n",
    "        embedded_email = copy.copy(email)\n",
    "        del embedded_email[\"clean_subj\"]\n",
    "        del embedded_email[\"clean_body\"]\n",
    "        embedded_chain.append(embedded_email)\n",
    "    embedded_emails.append(embedded_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"emails\"] = embedded_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(data[\"names\"])\n",
    "X_subj_sender = np.zeros((dim, num_nodes))\n",
    "X_body_sender = np.zeros((dim, num_nodes))\n",
    "X_subj_recip  = np.zeros((dim, num_nodes))\n",
    "X_body_recip  = np.zeros((dim, num_nodes))\n",
    "X_subj_cc     = np.zeros((dim, num_nodes))\n",
    "X_body_cc     = np.zeros((dim, num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_counts = np.zeros(num_nodes)\n",
    "recip_counts = np.zeros(num_nodes)\n",
    "cc_counts = np.zeros(num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chain in embedded_emails:\n",
    "    for email in chain:\n",
    "        subj_emb = email['subject_embedding']\n",
    "        body_emb = email['body_embedding']\n",
    "        \n",
    "        sender = email['sender']\n",
    "        X_subj_sender[:, sender] += subj_emb\n",
    "        X_body_sender[:, sender] += body_emb\n",
    "        sender_counts[sender] += 1\n",
    "        \n",
    "        for recip in email['recipients']:\n",
    "            X_subj_recip[:, recip] += subj_emb\n",
    "            X_body_recip[:, recip] += body_emb\n",
    "            recip_counts[recip] += 1\n",
    "        \n",
    "        for cc in email['cc']:\n",
    "            X_subj_cc[:, cc] += subj_emb\n",
    "            X_body_cc[:, cc] += body_emb\n",
    "            cc_counts[cc] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(num_nodes):\n",
    "    if sender_counts[j] > 0:\n",
    "        X_subj_sender[:, j] /= sender_counts[j]\n",
    "        X_body_sender[:, j] /= sender_counts[j]\n",
    "    if recip_counts[j] > 0:\n",
    "        X_subj_recip[:, j] /= recip_counts[j]\n",
    "        X_body_recip[:, j] /= recip_counts[j]\n",
    "    if cc_counts[j] > 0:\n",
    "        X_subj_cc[:, j] /= cc_counts[j]\n",
    "        X_body_cc[:, j] /= cc_counts[j]\n",
    "        \n",
    "X = np.vstack((X_subj_sender, X_body_sender,\n",
    "               X_subj_recip, X_body_recip,\n",
    "               X_subj_cc, X_body_cc))\n",
    "\n",
    "data['node_features'] = [list(X[:, j]) for j in range(num_nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fauci-email-data-w2v.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
